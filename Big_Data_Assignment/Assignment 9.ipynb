{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9857cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Scenario: You are building a microservices-based application using Docker. Design a Docker Compose file that sets up three containers: a web server container, a database container, and a cache container. Ensure that the containers can communicate with each other properly.\n",
    "2. Scenario: You want to scale your Docker containers dynamically based on the incoming traffic. Write a Python script that utilizes Docker SDK to monitor the CPU usage of a container and automatically scales the number of replicas based on a threshold.\n",
    "3. Scenario: You have a Docker image stored on a private registry. Develop a script in Bash that authenticates with the registry, pulls the latest version of the image, and runs a container based on that image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1706ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's an example of a Docker Compose file that sets up three containers: a web server container, a database container, and a cache container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "version: '3'\n",
    "services:\n",
    "  web:\n",
    "    image: your-web-image\n",
    "    ports:\n",
    "      - 80:80\n",
    "    depends_on:\n",
    "      - db\n",
    "      - cache\n",
    "    networks:\n",
    "      - mynetwork\n",
    "\n",
    "  db:\n",
    "    image: your-db-image\n",
    "    networks:\n",
    "      - mynetwork\n",
    "\n",
    "  cache:\n",
    "    image: your-cache-image\n",
    "    networks:\n",
    "      - mynetwork\n",
    "\n",
    "networks:\n",
    "  mynetwork:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's an example Python script that uses the Docker SDK to monitor the CPU usage of a container and scales the number of replicas based on a threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea056bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker\n",
    "import time\n",
    "\n",
    "client = docker.from_env()\n",
    "\n",
    "def get_cpu_usage(container):\n",
    "    stats = container.stats(stream=False)\n",
    "    return stats['cpu_stats']['cpu_usage']['total_usage']\n",
    "\n",
    "def scale_replicas(service, replicas):\n",
    "    service.scale(replicas=replicas)\n",
    "\n",
    "def monitor_cpu(container_name, threshold, scale_up_replicas, scale_down_replicas):\n",
    "    container = client.containers.get(container_name)\n",
    "    \n",
    "    while True:\n",
    "        cpu_usage = get_cpu_usage(container)\n",
    "        \n",
    "        if cpu_usage > threshold:\n",
    "            scale_replicas(container_name, scale_up_replicas)\n",
    "            print(f'Scaled up replicas of {container_name}')\n",
    "        elif cpu_usage < threshold and container.attrs['Spec']['Mode']['Replicated']['Replicas'] > 1:\n",
    "            scale_replicas(container_name, scale_down_replicas)\n",
    "            print(f'Scaled down replicas of {container_name}')\n",
    "        \n",
    "        time.sleep(5)\n",
    "\n",
    "# Example usage\n",
    "monitor_cpu('my-container', 80, 3, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a73ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's an example Bash script that authenticates with a private Docker registry, pulls the latest version of an image, and runs a container based on that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7277cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "DOCKER_REGISTRY_URL=\"your-registry-url\"\n",
    "DOCKER_IMAGE_NAME=\"your-image-name\"\n",
    "DOCKER_IMAGE_TAG=\"latest\"\n",
    "\n",
    "# Authenticate with the private registry\n",
    "docker login $DOCKER_REGISTRY_URL\n",
    "\n",
    "# Pull the latest version of the image\n",
    "docker pull $DOCKER_REGISTRY_URL/$DOCKER_IMAGE_NAME:$DOCKER_IMAGE_TAG\n",
    "\n",
    "# Run a container based on the image\n",
    "docker run -d $DOCKER_REGISTRY_URL/$DOCKER_IMAGE_NAME:$DOCKER_IMAGE_TAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba9a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2cb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Scenario: You have a data pipeline that requires executing a shell command as part of a task. Create an Airflow DAG that includes a BashOperator to execute a specific shell command.\n",
    "#2. Scenario: You want to create dynamic tasks in Airflow based on a list of inputs. Design an Airflow DAG that generates tasks dynamically using PythonOperator, where each task processes an element from the input list.\n",
    "#3. Scenario: You need to set up a complex task dependency in Airflow, where Task B should start only if Task A has successfully completed. Implement this dependency using the \"TriggerDagRunOperator\" in Airflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's an example of an Airflow DAG that includes a BashOperator to execute a shell command as part of a task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b165d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "    'start_date': datetime(2023, 7, 15)\n",
    "}\n",
    "\n",
    "dag = DAG('shell_command_dag', default_args=default_args, schedule_interval=None)\n",
    "\n",
    "task1 = BashOperator(\n",
    "    task_id='execute_shell_command',\n",
    "    bash_command='your_shell_command_here',\n",
    "    dag=dag\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a2051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's an example of an Airflow DAG that generates tasks dynamically using PythonOperator, where each task processes an element from the input list:\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "    'start_date': datetime(2023, 7, 15)\n",
    "}\n",
    "\n",
    "def process_element(element):\n",
    "    # Task logic for processing an element\n",
    "    print(f'Processing element: {element}')\n",
    "\n",
    "dag = DAG('dynamic_task_dag', default_args=default_args, schedule_interval=None)\n",
    "\n",
    "input_list = [1, 2, 3, 4, 5]\n",
    "\n",
    "for element in input_list:\n",
    "    task = PythonOperator(\n",
    "        task_id=f'process_element_{element}',\n",
    "        python_callable=process_element,\n",
    "        op_args=[element],\n",
    "        dag=dag\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ac8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To set up a complex task dependency in Airflow, where Task B should start only if Task A has successfully completed, you can use the \"TriggerDagRunOperator\" along with a custom TriggerRule.\n",
    "from airflow import DAG\n",
    "from airflow.operators.dagrun_operator import TriggerDagRunOperator\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "    'start_date': datetime(2023, 7, 15)\n",
    "}\n",
    "\n",
    "dag = DAG('complex_dependency_dag', default_args=default_args, schedule_interval=None)\n",
    "\n",
    "task_a = DummyOperator(\n",
    "    task_id='task_a',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "task_b = TriggerDagRunOperator(\n",
    "    task_id='task_b',\n",
    "    trigger_dag_id='complex_dependency_dag',\n",
    "    dag=dag,\n",
    "    trigger_rule='all_success'\n",
    ")\n",
    "\n",
    "task_a >> task_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Scenario: You want to import data from an Oracle database into Hadoop using Sqoop, but you only need to import specific columns from a specific table. Write a Sqoop command that performs the import, including the necessary arguments for column selection and table mapping.\n",
    "#2. Scenario: You have a requirement to perform an incremental import of data from a MySQL database into Hadoop using Sqoop. Design a Sqoop command that imports only the new or updated records since the last import.\n",
    "#3. Scenario: You need to export data from Hadoop to a Microsoft SQL Server database using Sqoop. Develop a Sqoop command that exports the data, considering factors like database connection details, table mapping, and appropriate data types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8249ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's an example of a Sqoop command that imports specific columns from a specific table in an Oracle database into Hadoop:\n",
    "sqoop import \\\n",
    "  --connect jdbc:oracle:thin:@<oracle_host>:<oracle_port>:<oracle_sid> \\\n",
    "  --username <username> \\\n",
    "  --password <password> \\\n",
    "  --table <table_name> \\\n",
    "  --columns \"<column1>,<column2>,<column3>\" \\\n",
    "  --target-dir <target_directory>\n",
    "#To perform an incremental import of data from a MySQL database into Hadoop using Sqoop, you can use the --incremental and --last-value options.\n",
    "sqoop import \\\n",
    "  --connect jdbc:mysql://<mysql_host>:<mysql_port>/<database> \\\n",
    "  --username <username> \\\n",
    "  --password <password> \\\n",
    "  --table <table_name> \\\n",
    "  --incremental append \\\n",
    "  --check-column <timestamp_column> \\\n",
    "  --last-value <last_imported_value> \\\n",
    "  --target-dir <target_directory>\n",
    "#To export data from Hadoop to a Microsoft SQL Server database using Sqoop, you can use the export command.\n",
    "sqoop export \\\n",
    "  --connect \"jdbc:sqlserver://<sql_server_host>:<sql_server_port>;database=<database>;username=<username>;password=<password>\" \\\n",
    "  --table <table_name> \\\n",
    "  --export-dir <export_directory> \\\n",
    "  --input-fields-terminated-by ',' \\\n",
    "  --input-lines-terminated-by '\\n'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
