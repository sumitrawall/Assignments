{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52010ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a Python program to read a Hadoop configuration file and display the core components of Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a264ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62602a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "def display_core_components(config_file_path):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file_path)\n",
    "\n",
    "    if 'core-site' in config:\n",
    "        core_components = config['core-site'].get('fs.defaultFS', '').split(\"://\")[0]\n",
    "        print(\"Core components of Hadoop:\")\n",
    "        print(core_components)\n",
    "    else:\n",
    "        print(\"No 'core-site' section found in the configuration file.\")\n",
    "\n",
    "# Specify the path to your Hadoop configuration file\n",
    "config_file_path = '/path/to/hadoop/config/file.xml'\n",
    "\n",
    "# Call the function to display the core components\n",
    "display_core_components(config_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f18533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def calculate_directory_size(hdfs_url, hdfs_directory):\n",
    "    # Create an HDFS client\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    # Retrieve the file status of the directory\n",
    "    directory_status = client.status(hdfs_directory)\n",
    "\n",
    "    # Check if the directory is a directory\n",
    "    if not directory_status['type'] == 'DIRECTORY':\n",
    "        print(\"Error: Specified path is not a directory.\")\n",
    "        return None\n",
    "\n",
    "    # Calculate the total size of the files in the directory\n",
    "    total_size = 0\n",
    "\n",
    "    for root, dirs, files in client.walk(hdfs_directory):\n",
    "        for file in files:\n",
    "            file_path = f\"{root}/{file}\"\n",
    "            file_status = client.status(file_path)\n",
    "            total_size += file_status['length']\n",
    "\n",
    "    return total_size\n",
    "\n",
    "# Specify the HDFS URL and directory path\n",
    "hdfs_url = 'http://localhost:9870'\n",
    "hdfs_directory = '/user/hadoop/data'\n",
    "\n",
    "# Call the function to calculate the total file size\n",
    "total_size = calculate_directory_size(hdfs_url, hdfs_directory)\n",
    "\n",
    "if total_size is not None:\n",
    "    print(\"Total file size:\", total_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "WORD_REGEX = re.compile(r\"\\b\\w+\\b\")\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_top_n)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        words = WORD_REGEX.findall(line)\n",
    "        for word in words:\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    def reducer_find_top_n(self, _, word_count_pairs):\n",
    "        top_n = Counter()\n",
    "        for count, word in word_count_pairs:\n",
    "            top_n[word] = count\n",
    "            if len(top_n) > N:\n",
    "                top_n = Counter(top_n.most_common(N))\n",
    "        for word, count in top_n.most_common(N):\n",
    "            yield word, count\n",
    "\n",
    "\n",
    "# Specify the path to your large text file\n",
    "input_file_path = '/path/to/large/text/file.txt'\n",
    "# Specify the value of N for the top N most frequent words\n",
    "N = 10\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TopNWords.run(args=[input_file_path])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a580a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Specify the URL of the NameNode's REST API\n",
    "namenode_url = 'http://<namenode-hostname>:<port>/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus'\n",
    "\n",
    "# Specify the URL of the DataNodes' REST API\n",
    "datanodes_url = 'http://<datanode-hostname>:<port>/jmx?qry=Hadoop:service=DataNode,name=DataNodeStatus'\n",
    "\n",
    "def check_namenode_health():\n",
    "    response = requests.get(namenode_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        health_status = data['beans'][0]['State']\n",
    "        return health_status\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "def check_datanodes_health():\n",
    "    response = requests.get(datanodes_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        datanodes = data['beans']\n",
    "        health_status = {dn['name']: dn['DatanodeState'] for dn in datanodes}\n",
    "        return health_status\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "# Check the health status of the NameNode\n",
    "namenode_health = check_namenode_health()\n",
    "print(\"NameNode health status:\", namenode_health)\n",
    "\n",
    "# Check the health status of the DataNodes\n",
    "datanodes_health = check_datanodes_health()\n",
    "print(\"DataNodes health status:\")\n",
    "for datanode, status in datanodes_health.items():\n",
    "    print(f\"{datanode}: {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Develop a Python program that lists all the files and directories in a specific HDFS path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def list_hdfs_path(hdfs_url, hdfs_path):\n",
    "    client = InsecureClient(hdfs_url)\n",
    "    file_list = client.list(hdfs_path, status=True)\n",
    "\n",
    "    for file_info in file_list:\n",
    "        file_path = file_info['path']\n",
    "        file_type = file_info['type']\n",
    "        print(f\"{file_type}: {file_path}\")\n",
    "\n",
    "# Specify the HDFS URL and the path you want to list\n",
    "hdfs_url = 'http://localhost:9870'\n",
    "hdfs_path = '/user/hadoop/data'\n",
    "\n",
    "# Call the function to list the files and directories\n",
    "list_hdfs_path(hdfs_url, hdfs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa569cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def analyze_storage_utilization(hdfs_url):\n",
    "    client = InsecureClient(hdfs_url)\n",
    "    datanodes = client.list('/datanode')\n",
    "\n",
    "    storage_utilization = {}\n",
    "    for datanode in datanodes:\n",
    "        datanode_info = client.status(f'/datanode/{datanode}')\n",
    "        storage_utilization[datanode] = datanode_info['length']\n",
    "\n",
    "    if not storage_utilization:\n",
    "        print(\"No DataNodes found.\")\n",
    "        return\n",
    "\n",
    "    # Find DataNode with highest storage capacity\n",
    "    max_storage_datanode = max(storage_utilization, key=storage_utilization.get)\n",
    "    max_storage_capacity = storage_utilization[max_storage_datanode]\n",
    "\n",
    "    # Find DataNode with lowest storage capacity\n",
    "    min_storage_datanode = min(storage_utilization, key=storage_utilization.get)\n",
    "    min_storage_capacity = storage_utilization[min_storage_datanode]\n",
    "\n",
    "    print(\"Storage Utilization Analysis:\")\n",
    "    print(\"DataNode with Highest Storage Capacity:\")\n",
    "    print(f\"  - DataNode: {max_storage_datanode}\")\n",
    "    print(f\"  - Storage Capacity: {max_storage_capacity} bytes\")\n",
    "\n",
    "    print(\"DataNode with Lowest Storage Capacity:\")\n",
    "    print(f\"  - DataNode: {min_storage_datanode}\")\n",
    "    print(f\"  - Storage Capacity: {min_storage_capacity} bytes\")\n",
    "\n",
    "# Specify the HDFS URL\n",
    "hdfs_url = 'http://localhost:9870'\n",
    "\n",
    "# Call the function to analyze storage utilization\n",
    "analyze_storage_utilization(hdfs_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd1f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bad731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Specify the URL of the YARN ResourceManager API\n",
    "resource_manager_url = 'http://<resource-manager-hostname>:<port>/ws/v1/cluster'\n",
    "\n",
    "# Specify the path to your Hadoop job JAR file\n",
    "hadoop_job_jar_path = '/path/to/hadoop/job.jar'\n",
    "\n",
    "# Specify the main class of your Hadoop job\n",
    "hadoop_job_main_class = 'com.example.HadoopJobMain'\n",
    "\n",
    "# Specify the input and output paths for your Hadoop job\n",
    "input_path = '/path/to/input'\n",
    "output_path = '/path/to/output'\n",
    "\n",
    "def submit_hadoop_job():\n",
    "    # Submit the Hadoop job\n",
    "    data = {\n",
    "        'application-id': 'application_123456789_0001',  # Unique application ID for tracking the job\n",
    "        'application-name': 'HadoopJob',\n",
    "        'am-container-spec': {\n",
    "            'local-resources': {\n",
    "                'entry': [\n",
    "                    {\n",
    "                        'key': 'job.jar',\n",
    "                        'value': {\n",
    "                            'resource': hadoop_job_jar_path,\n",
    "                            'type': 'FILE'\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            'commands': {\n",
    "                'command': f'hadoop jar job.jar {hadoop_job_main_class} {input_path} {output_path}'\n",
    "            },\n",
    "            'memory': 1024,\n",
    "            'vcores': 1\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(f\"{resource_manager_url}/apps\", json=data)\n",
    "    if response.status_code == 202:\n",
    "        return response.json()['application-id']\n",
    "    else:\n",
    "        print(\"Failed to submit the Hadoop job.\")\n",
    "        return None\n",
    "\n",
    "def monitor_job_progress(application_id):\n",
    "    # Monitor the progress of the Hadoop job\n",
    "    while True:\n",
    "        response = requests.get(f\"{resource_manager_url}/apps/{application_id}\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            state = data['app']['state']\n",
    "            final_status = data['app']['finalStatus']\n",
    "            progress = data['app']['progress']\n",
    "            print(f\"Job state: {state}, Final status: {final_status}, Progress: {progress}%\")\n",
    "\n",
    "            if final_status != 'UNDEFINED':\n",
    "                break\n",
    "        else:\n",
    "            print(\"Failed to retrieve job status.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "def retrieve_job_output():\n",
    "    # Retrieve the output of the Hadoop job\n",
    "    response = requests.get(f\"{resource_manager_url}/apps/{application_id}/state\")\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        final_status = data['app']['finalStatus']\n",
    "\n",
    "        if final_status == 'SUCCEEDED':\n",
    "            # Retrieve the job output from HDFS or any other storage location\n",
    "            print(\"Job completed successfully.\")\n",
    "            # Add code to retrieve the output based on your storage configuration\n",
    "        else:\n",
    "            print(\"Job did not complete successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve job status.\")\n",
    "\n",
    "# Submit the Hadoop job\n",
    "application_id = submit_hadoop_job()\n",
    "\n",
    "if application_id:\n",
    "    # Monitor the progress of the Hadoop job\n",
    "    monitor_job_progress(application_id)\n",
    "\n",
    "    # Retrieve the output of the Hadoop job\n",
    "    retrieve_job_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05123fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Specify the URL of the YARN ResourceManager API\n",
    "resource_manager_url = 'http://<resource-manager-hostname>:<port>/ws/v1/cluster'\n",
    "\n",
    "# Specify the path to your Hadoop job JAR file\n",
    "hadoop_job_jar_path = '/path/to/hadoop/job.jar'\n",
    "\n",
    "# Specify the main class of your Hadoop job\n",
    "hadoop_job_main_class = 'com.example.HadoopJobMain'\n",
    "\n",
    "# Specify the input and output paths for your Hadoop job\n",
    "input_path = '/path/to/input'\n",
    "output_path = '/path/to/output'\n",
    "\n",
    "# Specify the resource requirements for the Hadoop job\n",
    "memory_mb = 2048\n",
    "vcores = 2\n",
    "\n",
    "def submit_hadoop_job():\n",
    "    # Submit the Hadoop job\n",
    "    data = {\n",
    "        'application-id': 'application_123456789_0001',  # Unique application ID for tracking the job\n",
    "        'application-name': 'HadoopJob',\n",
    "        'am-container-spec': {\n",
    "            'local-resources': {\n",
    "                'entry': [\n",
    "                    {\n",
    "                        'key': 'job.jar',\n",
    "                        'value': {\n",
    "                            'resource': hadoop_job_jar_path,\n",
    "                            'type': 'FILE'\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            'commands': {\n",
    "                'command': f'hadoop jar job.jar {hadoop_job_main_class} {input_path} {output_path}'\n",
    "            },\n",
    "            'resource': {\n",
    "                'memory': memory_mb,\n",
    "                'vCores': vcores\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(f\"{resource_manager_url}/apps\", json=data)\n",
    "    if response.status_code == 202:\n",
    "        return response.json()['application-id']\n",
    "    else:\n",
    "        print(\"Failed to submit the Hadoop job.\")\n",
    "        return None\n",
    "\n",
    "def track_resource_usage(application_id):\n",
    "    # Track the resource usage of the Hadoop job\n",
    "    while True:\n",
    "        response = requests.get(f\"{resource_manager_url}/apps/{application_id}/appattempts\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            attempts = data['appAttempts']['appAttempt']\n",
    "            current_attempt = attempts[-1]\n",
    "            container_id = current_attempt['containerId']\n",
    "\n",
    "            container_response = requests.get(f\"{resource_manager_url}/cluster/apps/{application_id}/appattempts/{container_id}/containers/{container_id}\")\n",
    "            if container_response.status_code == 200:\n",
    "                container_data = container_response.json()\n",
    "                container_state = container_data['container']['state']\n",
    "                allocated_resources = container_data['container']['allocatedResources']\n",
    "                allocated_memory_mb = allocated_resources['memory']\n",
    "                allocated_vcores = allocated_resources['vCores']\n",
    "\n",
    "                print(f\"Container state: {container_state}\")\n",
    "                print(f\"Allocated memory: {allocated_memory_mb} MB\")\n",
    "                print(f\"Allocated vCores: {allocated_vcores}\")\n",
    "            else:\n",
    "                print(\"Failed to retrieve container information.\")\n",
    "\n",
    "            if container_state == 'COMPLETE':\n",
    "                break\n",
    "        else:\n",
    "            print(\"Failed to retrieve application attempts.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "# Submit the Hadoop job\n",
    "application_id = submit_hadoop_job()\n",
    "\n",
    "if application_id:\n",
    "    # Track the resource usage of the Hadoop job\n",
    "    track_resource_usage(application_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886069bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00615e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "\n",
    "class MapReduceJob(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(MapReduceJob, self).configure_args()\n",
    "        self.add_passthru_arg('--split-size', default=64, type=int, help='Input split size in megabytes')\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield None, len(line)\n",
    "\n",
    "    def reducer(self, _, lengths):\n",
    "        yield None, sum(lengths)\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper, reducer=self.reducer)\n",
    "        ]\n",
    "\n",
    "# Specify the input file path\n",
    "input_file_path = '/path/to/input/file.txt'\n",
    "\n",
    "# Specify the different input split sizes to compare\n",
    "split_sizes = [64, 128, 256]\n",
    "\n",
    "# Run the MapReduce job with different input split sizes and compare the execution times\n",
    "for split_size in split_sizes:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run the MapReduce job\n",
    "    job = MapReduceJob(args=[input_file_path, f'--split-size={split_size}'])\n",
    "    with job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "        # Collect the output\n",
    "        output = [line.strip() for line in runner.cat_output()]\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the execution time\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Input Split Size: {split_size} MB\")\n",
    "    print(\"Output:\", output)\n",
    "    print(\"Execution Time:\", execution_time, \"seconds\")\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
